{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Hyperparameter Tuning\n\nThis notebook implements and tunes a Logistic Regression classifier for wildfire detection using NASA satellite measurement data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"../data/final_combined_dataset.csv\"\n",
    "RANDOM_STATE = 1234\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop([\"FIRE\", \"Date\"], axis=1)\n",
    "y = data['FIRE']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, data_train, data_test = train_test_split(\n",
    "    X, y, data, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Display dataset shapes\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"  Training set: {X_train.shape}\")\n",
    "print(f\"  Test set:     {X_test.shape}\")\n",
    "print(f\"\\nLabel shapes:\")\n",
    "print(f\"  Training labels: {y_train.shape}\")\n",
    "print(f\"  Test labels:     {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn imports\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    roc_curve,\n    confusion_matrix\n)\n\n\ndef evaluate_pipelines(pipelines, X_train, y_train, X_evaluate, y_evaluate):\n    \"\"\"\n    Evaluate multiple ML pipelines and return performance metrics.\n    \n    Parameters:\n    -----------\n    pipelines : list of tuples\n        List of (name, pipeline) tuples to evaluate\n    X_train : array-like\n        Training features\n    y_train : array-like\n        Training labels\n    X_evaluate : array-like\n        Evaluation features\n    y_evaluate : array-like\n        Evaluation labels\n    \n    Returns:\n    --------\n    pd.DataFrame\n        DataFrame containing performance metrics for each pipeline\n    \"\"\"\n    rows = []\n    for name, pipeline in pipelines:\n        # Fit pipeline on training data\n        pipeline.fit(X_train, y_train)\n        \n        # Generate predictions\n        train_preds = pipeline.predict(X_train)\n        eval_preds = pipeline.predict(X_evaluate)\n        \n        # Get probabilities for ROC-AUC calculation\n        train_probs = pipeline.predict_proba(X_train)[:, 1]\n        eval_probs = pipeline.predict_proba(X_evaluate)[:, 1]\n        \n        # Calculate metrics\n        rows.append({\n            'name': name,\n            'Training Accuracy': accuracy_score(y_train, train_preds),\n            'Eval Accuracy': accuracy_score(y_evaluate, eval_preds),\n            'Training Recall': recall_score(y_train, train_preds),\n            'Eval Recall': recall_score(y_evaluate, eval_preds),\n            'Training Precision': precision_score(y_train, train_preds),\n            'Eval Precision': precision_score(y_evaluate, eval_preds),\n            'Training F1': f1_score(y_train, train_preds),\n            'Eval F1': f1_score(y_evaluate, eval_preds),\n            'Training ROC': roc_auc_score(y_train, train_probs),\n            'Eval ROC': roc_auc_score(y_evaluate, eval_probs)\n        })\n    \n    return pd.DataFrame(rows)\n\n\ndef plot_confusion_matrix(y_true, y_pred, clf_name):\n    \"\"\"\n    Plot confusion matrix for model predictions.\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        True labels\n    y_pred : array-like\n        Predicted labels\n    clf_name : str\n        Name of the classifier for the plot title\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='g', cmap='viridis')\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    plt.title(f'{clf_name} Confusion Matrix', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_roc_curve(y_true, y_probs, clf_name=\"\"):\n    \"\"\"\n    Plot ROC curve and display AUC score.\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        True labels\n    y_probs : array-like\n        Predicted probabilities for the positive class\n    clf_name : str, optional\n        Name of the classifier for the plot title\n    \"\"\"\n    auc = roc_auc_score(y_true, y_probs)\n    fpr, tpr, _ = roc_curve(y_true, y_probs)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, label=f\"AUC = {auc:.5f}\", linewidth=2)\n    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', alpha=0.5)\n    plt.xlabel(\"False Positive Rate\", fontsize=12)\n    plt.ylabel(\"True Positive Rate\", fontsize=12)\n    title = f\"ROC Curve - {clf_name}\" if clf_name else \"ROC Curve\"\n    plt.title(title, fontsize=14, fontweight='bold')\n    plt.legend(fontsize=11)\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n\n# Initialize baseline Logistic Regression model\nlogistic_regression = make_pipeline(\n    LogisticRegression(random_state=RANDOM_STATE, max_iter=10000)\n)\n\n# Evaluate baseline model\nbaseline_results = evaluate_pipelines(\n    [('Logistic Regression', logistic_regression)],\n    X_train, y_train, X_test, y_test\n)\ndisplay(baseline_results)\n\n# Generate visualizations\nprint(\"",
    "Generating visualizations for Logistic Regression...\")\nlogistic_regression.fit(X_train, y_train)\ny_pred = logistic_regression.predict(X_test)\ny_probs = logistic_regression.predict_proba(X_test)[:, 1]\nplot_confusion_matrix(y_test, y_pred, \"Logistic Regression\")\nplot_roc_curve(y_test, y_probs, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n\nTune Logistic Regression based on the following parameters:\n- C (regularization strength)\n- tol (tolerance)\n\nUtilize Cross-Validation for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for Logistic Regression\n",
    "lr_param_grid = {\n",
    "    'tol': np.linspace(0.0001, 0.001, num=10, dtype=float),\n",
    "    'C': np.linspace(0.01, 0.1, num=5, dtype=float)\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "lr_grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc_ovo',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Performing grid search for Logistic Regression...\")\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters\n",
    "print(f\"\\nBest Parameters: {lr_grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {lr_grid_search.best_score_:.6f}\")\n",
    "\n",
    "# Visualize grid search results\n",
    "results_df = pd.DataFrame(lr_grid_search.cv_results_)\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_tol',\n",
    "    columns='param_C'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", cbar_kws={'label': 'ROC-AUC Score'})\n",
    "plt.title('Logistic Regression Grid Search CV Results (ROC-AUC)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('C (Regularization Strength)', fontsize=12)\n",
    "plt.ylabel('Tolerance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n\nCompare base and tuned Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters\n",
    "LR_BEST_C = lr_grid_search.best_params_['C']\n",
    "LR_BEST_TOL = lr_grid_search.best_params_['tol']\n",
    "\n",
    "# Create base and tuned models\n",
    "base_lr = make_pipeline(LogisticRegression(random_state=RANDOM_STATE, max_iter=10000))\n",
    "tuned_lr = make_pipeline(LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=10000,\n",
    "    C=LR_BEST_C,\n",
    "    tol=LR_BEST_TOL\n",
    "))\n",
    "\n",
    "# Evaluate models\n",
    "print(\"Comparing base and tuned Logistic Regression models:\")\n",
    "comparison_results = evaluate_pipelines([\n",
    "    ('Base Logistic Regression', base_lr),\n",
    "    ('Tuned Logistic Regression', tuned_lr)\n",
    "], X_train, y_train, X_test, y_test)\n",
    "display(comparison_results)\n",
    "\n",
    "# Generate visualizations\n",
    "models_to_plot = [\n",
    "    ('Base Logistic Regression', base_lr),\n",
    "    ('Tuned Logistic Regression', tuned_lr)\n",
    "]\n",
    "\n",
    "for name, clf in models_to_plot:\n",
    "    print(f\"\\nGenerating visualizations for {name}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "    plot_confusion_matrix(y_test, y_pred, name)\n",
    "    plot_roc_curve(y_test, y_probs, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Hyperparameter Tuning\n\nEvaluate the impact of feature scaling on Logistic Regression performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate impact of feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Create base and scaled models\n",
    "base_lr = make_pipeline(LogisticRegression(random_state=RANDOM_STATE, max_iter=10000))\n",
    "scaled_lr = make_pipeline(scaler, LogisticRegression(random_state=RANDOM_STATE, max_iter=10000))\n",
    "\n",
    "# Evaluate models\n",
    "print(\"Comparing base and scaled Logistic Regression models:\")\n",
    "scaling_results = evaluate_pipelines([\n",
    "    ('Base Logistic Regression', base_lr),\n",
    "    ('Scaled Logistic Regression', scaled_lr)\n",
    "], X_train, y_train, X_test, y_test)\n",
    "display(scaling_results)\n",
    "\n",
    "# Generate visualizations\n",
    "models_to_plot = [\n",
    "    ('Base Logistic Regression', base_lr),\n",
    "    ('Scaled Logistic Regression', scaled_lr)\n",
    "]\n",
    "\n",
    "for name, clf in models_to_plot:\n",
    "    print(f\"\\nGenerating visualizations for {name}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "    plot_confusion_matrix(y_test, y_pred, name)\n",
    "    plot_roc_curve(y_test, y_probs, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled and Tuned Model\n\nCombine feature scaling with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaled and tuned model\n",
    "tuned_lr = make_pipeline(\n",
    "    scaler,\n",
    "    LogisticRegression(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=10000,\n",
    "        C=LR_BEST_C,\n",
    "        tol=LR_BEST_TOL\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluate all model variants\n",
    "print(\"Comparing all Logistic Regression model variants:\")\n",
    "final_results = evaluate_pipelines([\n",
    "    ('Base Logistic Regression', base_lr),\n",
    "    ('Scaled Logistic Regression', scaled_lr),\n",
    "    ('Scaled and Tuned Logistic Regression', tuned_lr)\n",
    "], X_train, y_train, X_test, y_test)\n",
    "display(final_results)\n",
    "\n",
    "# Generate visualizations for all models\n",
    "models_to_plot = [\n",
    "    ('Base Logistic Regression', base_lr),\n",
    "    ('Scaled Logistic Regression', scaled_lr),\n",
    "    ('Scaled and Tuned Logistic Regression', tuned_lr)\n",
    "]\n",
    "\n",
    "for name, clf in models_to_plot:\n",
    "    print(f\"\\nGenerating visualizations for {name}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "    plot_confusion_matrix(y_test, y_pred, name)\n",
    "    plot_roc_curve(y_test, y_probs, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nUsing cross-validation to tune the hyperparameters of the Logistic Regression model, we note improvements in the ROC-AUC score. The tuned model shows enhanced performance for wildfire risk prediction, which is our main goal for forecasting risk levels for wildfire outbreak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis using permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance for Logistic Regression\n",
    "print(\"Calculating permutation importance for Logistic Regression...\")\n",
    "lr_perm = permutation_importance(\n",
    "    estimator=tuned_lr,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    n_repeats=15,\n",
    "    scoring='roc_auc',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Format results as DataFrame\n",
    "lr_perm_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Importance': lr_perm.importances_mean,\n",
    "    'Std': lr_perm.importances_std\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nLogistic Regression Feature Importance:\")\n",
    "print(\"=\" * 60)\n",
    "display(lr_perm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Logistic Regression feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(lr_perm_df['Feature'], lr_perm_df['Importance'], color='skyblue', alpha=0.8)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Logistic Regression Feature Importance (Permutation Importance)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()  # Most important at the top\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}